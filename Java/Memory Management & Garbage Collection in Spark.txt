Memory Management
The main resources Spark needs are CPU and memory. We have already seen 
how CPUs together with partitions infuence the parallelism level of our appli-
cations. Now we will focus on memory usage and the ways we can improve 
performance by tuning the memory. Next, we will describe situations when 
Spark uses the cluster’s memory.
During the shuffe phases, Spark keeps in-memory intermediary results 
used for aggregations and co-groups. The amount of memory of the in-memory 
maps used for shuffing is limited by the value passed to the spark.shuffle
.memoryFraction property only if the spark.shuffle.spill is set to true. If the 
data exceeds this limit, it will be spilled to disk. Each reducer creates a buffer 
in which it will fetch the output data of each map task. This buffer is kept in 
memory and has a fxed size per reducer, so unless you have a lot of memory, 
you should keep it small. The size of this buffer can be specifed through the 
spark.reducer.maxSizeInFlight.
Spark also makes good use of memory when you cache the RDDs. The amount 
of memory used for persisting datasets is limited to a fraction of the overall Java 
heap. This limit is set through the spark.storage.memoryFraction.
Of course, Spark also uses memory when executing the user code, which 
might allocate memory for huge objects. The user code will take the rest of the 
heap after the one used for persistence and shuffing is allocated.
If you don’t specify otherwise, by default, Spark will give 60 percent from the 
Java heap to persistence, 20 percent to shuffes, and its remaining 20 percent for 
executing the user code. You can fne-tune these percentages if you think they 
don’t ft your use case.
Because Spark stores this large amount of data in-memory, it strongly depends 
on Java memory management and garbage collection. Knowing how to properly 
tune the GC improves performance.
Before we discuss how garbage collection infuences Spark’s application eff-
ciency, we should frst remember how garbage collection works in Java.
Garbage Collection
Based on how long an object’s lifecycle is, it is stored either in the Young or 
Old generations in which the Java heap space is divided. Short-lived objects are 
stored in the Young generation while the long-lived ones are stored in the Old 
generation. The Young generation region is also divided into three sections: 
Eden and two Survivor regions.
 When an object is frst created, it will be written in Eden. When Eden flls up, 
a minor GC (garbage collection) is triggered. During this phase all of the alive 
objects from Eden and from SurvivorNo1 will be moved to SurvivorNo2, then 
the Survivors are swapped. When the alive objects from SurvivorNo2 are old 
enough or the space allocated for SurvivorNo2 is full, the objects are moved to 
the Old generation space. When the Old generation space is almost full, then a 
full GC is triggered. This is the moment when performance is highly impacted 
as the application threads are stopped while the objects in the Old generation 
are organized.
To measure the impact the GC has on your application you can edit the 
SPARK_JAVA_OPTS environment variable or you set the spark.executor
.extraJavaOptions property in SparkConf by adding:
-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
The detailed garbage collector log can be inspected in the executor’s log (on 
every worker node in the cluster). In this way we can track the entire GC activity. 
We can analyze when and why threads are paused, the average and maximum 
CPU time, the cleanup results, etc.
Based on these statistics we can fne-tune the GC. If we observe in the logs 
that the full garbage collection is called multiple times before the tasks fnish, 
this means that there is not enough memory to run the tasks. Also if we observe 
in logs that the old generation is almost full, this means that we might use too 
much memory for caching. In both cases we should decrease the memory used 
for caching.
On the other hand, we might observe that there are a lot of minor GCs and 
not so many full ones. In this case it might help to allocate more space for Eden. 
You should scale up the space with 4/3 to take into account also the space dedi-
cated to the survivors.
Garbage collection plays a very important role in Spark Streaming applica-
tions, because they require a low latency. Therefore long pauses generated by 
GC are unwanted. In this particular case, to keep the GC pauses constantly low, 
we recommend you use the concurrent mark-and-sweep GC on both driver 
and executors.
To summarize, if you observe that your application’s performance is decreas-
ing, you have to make sure that you are effciently using the heap memory for 
caching. If you leave more heap space for the program’s execution, garbage col-
lection will be more effcient. On the other hand if you allocate too much heap 
space for caching and you don’t take care how you use this space (where we 
excessively consume the memory) then you might end up with a large number 
of objects in the GC old generation, leading to a great performance loss. You 
can easily improve performance by explicitly cleaning the cached RDDs when 
you no longer need them.
