Random JavaTech


When Java source code is compiled, annotations can be processed by compiler plug-ins called annotation processors. 
Processors can produce informational messages or create additional Java source files or resources, which in turn may be compiled and processed, 
and also modify the annotated code itself. The Java compiler conditionally stores annotation metadata in the class files, 
if the annotation has a RetentionPolicy of CLASS or RUNTIME. 
Later, the JVM or other programs can look for the metadata to determine how to interact with the program elements or change their behavior.

In addition to processing an annotation using an annotation processor, a Java programmer can write their own code that uses reflections 
to process the annotation. Java SE 5 supports a new interface that is defined in the java.lang.reflect package. 
This package contains the interface called AnnotatedElement that is implemented by the Java reflection classes 
including Class, Constructor, Field, Method, and Package. 
The implementations of this interface are used to represent an annotated element of the program currently running in the Java Virtual Machine. 
This interface allows annotations to be read reflectively.

The AnnotatedElement interface provides access to annotations having RUNTIME retention. 
This access is provided by the getAnnotation, getAnnotations, and isAnnotationPresent methods. 
Because annotation types are compiled and stored in byte code files just like classes, the annotations returned by these methods can be queried just 
like any regular Java object. A complete example of processing an annotation is provided below:


A special case is the Java programming language, where annotations can be used as a special form of syntactic metadata in the source code.[1] 
Classes, methods, variables, parameters and packages may be annotated. 
The annotations can be embedded in class files generated by the compiler and may be retained by the Java virtual machine and thus influence 
the run-time behaviour of an application. 
It is possible to create meta-annotations out of the existing ones in Java.




http://en.wikipedia.org/wiki/JSR_250
JSR 250 is a Java Specification Request with the objective to develop annotations 
(that is, information about a software program that is not part of the program itself) 
for common semantic concepts in the Java SE and Java EE platforms that apply across a variety of individual technologies. 
It was envisioned that various JSRs would use annotations to enable a declarative style of programming. 
It would be especially valuable to have consistency within the Java EE component JSRs, 
but it is also valuable to allow consistency between Java EE and Java SE.


JSR 250 depends on JSR 175 and therefore also on Java SE 5.0
The annotations
Annotation name 	description
Generated 			Marks sources that have been generated
Resource 			Declares a reference to a resource, e.g. a database
Resources 			Container for multiple Resource annotations
PostConstruct 		Is used on methods that need to get executed after dependency injection is done to perform any initialization.
PreDestroy 			Is used on methods that are called before the instance is removed from the container
Priority 			Is used to indicate in what order the classes should be used. For, e.g., the Interceptors specification defines the use of priorities on interceptors to control the order in which interceptors are called.
RunAs 				Defines the role of the application during execution in a Java EE container
RolesAllowed 		Specifies the security roles permitted to access method(s) in an application.
PermitAll 			Specifies that all security roles are permitted to access the annotated method, or all methods in the annotated class.
DenyAll 			Specifies that no security roles are allowed to invoke the specified method(s).
DeclareRoles 		Used to specify the security roles by the application.
DataSourceDefinition 	Is used to define a container DataSource and be registered with JNDI. The DataSource may be configured by setting the annotation elements for commonly used DataSource properties.
ManagedBean 			Is used to declare a Managed Bean which are container managed objects that support a small set of basic services such as resource injection, lifecycle callbacks and interceptors.


Implementation

All non-Java EE JSR 250 annotations were added to the Java SE with version 6 (Generated, PostConstruct, PreDestroy, Resource, Resources). They are located 



==============================================================================================================
http://docs.spring.io/spring/docs/3.0.x/spring-framework-reference/html/beans.html

This chapter covers the Spring Framework implementation of the Inversion of Control (IoC) [1]principle. IoC is also known as dependency injection (DI). 
It is a process whereby objects define their dependencies, that is, the other objects they work with, only through constructor arguments, arguments to a 
factory method, or properties that are set on the object instance after it is constructed or returned from a factory method. 
The container then injects those dependencies when it creates the bean. 

This process is fundamentally the inverse, hence the name Inversion of Control (IoC), 
of the bean itself controlling the instantiation or location of its dependencies by using direct construction of classes, 
or a mechanism such as the Service Locator pattern.

The org.springframework.beans and org.springframework.context packages are the basis for Spring Framework's IoC container. 
The BeanFactory interface provides an advanced configuration mechanism capable of managing any type of object. 
ApplicationContext is a sub-interface of BeanFactory. 
It adds easier integration with Spring's AOP features; message resource handling (for use in internationalization), event publication; 
and application-layer specific contexts such as the WebApplicationContext for use in web applications.

In short, the BeanFactory provides the configuration framework and basic functionality, and the ApplicationContext adds more enterprise-specific functionality. 
The ApplicationContext is a complete superset of the BeanFactory




==============================================================================================================




public interface BeanFactory

The root interface for accessing a Spring bean container. This is the basic client view of a bean container; further interfaces such as ListableBeanFactory and ConfigurableBeanFactory are available for specific purposes.

This interface is implemented by objects that hold a number of bean definitions, each uniquely identified by a String name. Depending on the bean definition, the factory will return either an independent instance of a contained object (the Prototype design pattern), or a single shared instance (a superior alternative to the Singleton design pattern, in which the instance is a singleton in the scope of the factory). Which type of instance will be returned depends on the bean factory configuration: the API is the same. Since Spring 2.0, further scopes are available depending on the concrete application context (e.g. "request" and "session" scopes in a web environment).

The point of this approach is that the BeanFactory is a central registry of application components, and centralizes configuration of application components (no more do individual objects need to read properties files, for example). See chapters 4 and 11 of "Expert One-on-One J2EE Design and Development" for a discussion of the benefits of this approach.

Note that it is generally better to rely on Dependency Injection ("push" configuration) to configure application objects through setters or constructors, rather than use any form of "pull" configuration like a BeanFactory lookup. Spring's Dependency Injection functionality is implemented using this BeanFactory interface and its subinterfaces.

Normally a BeanFactory will load bean definitions stored in a configuration source (such as an XML document), and use the org.springframework.beans package to configure the beans. However, an implementation could simply return Java objects it creates as necessary directly in Java code. There are no constraints on how the definitions could be stored: LDAP, RDBMS, XML, properties file, etc. Implementations are encouraged to support references amongst beans (Dependency Injection).

In contrast to the methods in ListableBeanFactory, all of the operations in this interface will also check parent factories if this is a HierarchicalBeanFactory. If a bean is not found in this factory instance, the immediate parent factory will be asked. Beans in this factory instance are supposed to override beans of the same name in any parent factory.

Bean factory implementations should support the standard bean lifecycle interfaces as far as possible. The full set of initialization methods and their standard order is:
1. BeanNameAware's setBeanName
2. BeanClassLoaderAware's setBeanClassLoader
3. BeanFactoryAware's setBeanFactory
4. ResourceLoaderAware's setResourceLoader (only applicable when running in an application context)
5. ApplicationEventPublisherAware's setApplicationEventPublisher (only applicable when running in an application context)
6. MessageSourceAware's setMessageSource (only applicable when running in an application context)
7. ApplicationContextAware's setApplicationContext (only applicable when running in an application context)
8. ServletContextAware's setServletContext (only applicable when running in a web application context)
9. postProcessBeforeInitialization methods of BeanPostProcessors
10. InitializingBean's afterPropertiesSet
11. a custom init-method definition
12. postProcessAfterInitialization methods of BeanPostProcessors

On shutdown of a bean factory, the following lifecycle methods apply:
1. DisposableBean's destroy
2. a custom destroy-method definition
















CreateSpace.Algorithms.For.Interviews.1453792996.pdf

REDUCTION probkm of fiMing if om st?g is a rotation of the other,
e.g., "car" ?1d Harc"are rotatiORs of each other A I1aturd approach may
be to rotate the first strhgby everypomible offset aM ttmcomar4
wi? the second st?g. This algorithm would have quadratic ti? com­
plexity.
You may I1otice that this problem is quite s ?nilar to string search
which cm be domh1inear-tmer albeit mhg a somewhat complex alm
gorithm.So it would be I1aturd to try to reduce this problem to string
search.IndeedrifwecomatemtethesecondstringwithitselfaMsearcE
for the first stying h tke resulting string, we will find a match iff the two
original strhgs are rotatiOI1s of each other.This reduction yields a linear-
time algorithm for our problem;details are giveR iRProbkII15.4.
Usually you try to reduce your proble~ to an easier problem. But
sometmesr you need to reduce a problem bmWI1to be difficult to your
giveI1problem to show that your problem is difficult.Such probkms are
described in Chapter 6.

---------------------------------------
 map function will take a list of anything and another function that will be used to
transform each individual item it is passed.
the function returns the transformed collection .



----------------------------

	The Google File System



----------------------------



----------------------------

	Functional Programming for Java Developers

Functional programming, in its “purest” sense, is rooted in how functions, variables,
and values actually work in mathematics, which is different from how they typically
work in most programming languages.	
Combinatory Logic examines how combinators, which are essentially functions, combine to represent
a computation. 	
	
The Basic Principles of Functional Programming	Avoiding Mutable State
***	The first principle is the use of immutable values. 
Most programming languages don’t make a clear distinction between a value (i.e., the
contents of memory) and a variable that refers to it. In Java, we’ll use final to prohibit
variable reassignment, so we get objects that are immutable values.
Why should we avoid mutating values? First, allowing mutable values is what makes
multithreaded  programming  so  difficult.  If multiple  threads  can modify  the  same
shared value, you have to synchronize access to that value. This is quite tedious and
error-prone programming that even the experts find challenging [Goetz2006]. If you
make a value immutable, the synchronization problem disappears. Concurrent reading
is harmless, so multithreaded programming becomes far easier.
A second benefit of immutable values relates to program correctness in other ways. It
is harder to understand and exhaustively test code with mutable values, particularly if
mutations aren’t localized to one place. Some of the most difficult bugs to find in large
systems occur when state is modified non-locally, by client code that is located else-
where in the program.



----------------------------

		
	
	
-------------------------------------------

 the well-grounded  Java developer

make use of
Scala’s  novel  approach  to  object-orientation,  type  inference,  and  flexible
syntax  capabilities,  new  collections  classes  (including  natural  functional  pro-
gramming  style,  such  as  the map/filter  idioms),  and  the  actor-based  concur-
rency model	

Dynamic vs. static typing
In  languages with dynamic  typing, a variable can contain different  types at different
times. As an example, let’s look at a simple bit of code in a well-known dynamic lan-
guage,  JavaScript.  This  example  should  hopefully  be  comprehensible  even  if  you
don’t know the language in detail:
var answer = 40;
answer = answer + 2;
answer = "What is the answer? " + answer;
In  this  code,  the  variable  answer  starts  off  being  set  to  40,  which  is,  of  course,  a
numeric value. We then add 2 to it, giving 42. Then we change track slightly and make
answer hold a string value. This is a very common technique in a dynamic language,
and it causes no syntax errors.
 The JavaScript interpreter is also able to distinguish between the two uses of
the + operator. The first use of + is numeric addition—adding 2 to 40, whereas in the
following  line  the  interpreter  figures  out  from  context  that  the  developer  meant
string concatenation.
NOTE The  key  point  here  is  that  dynamic  typing  keeps  track  of  informa-
tion  about  what  sort  of  values  the  variables  contain  (for  example,  a  num-
ber  or  a  string),  and  static  typing  keeps  track  of  type  information  about
the variables.
Static typing can be a good fit for a compiled language because the type information
is all about the variables, not the values in them. This makes it much easier to reason
about potential type system violations at compile time.
 Dynamically typed languages carry type information on the values held in vari-
ables. This means  that  it’s much harder  to reason about  type violations because  the
information needed for that reasoning isn’t known until execution time.

------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------
BIG DATA Algorithms, Analytics,  
and Applications



Streaming Algorithms for 
Big Data Processing on 
Multicore Architecture


HaDoop fle system (HDFS) [1] and MapReduce are de facto standards in Big Data processing 
today. Although they are two separate technologies, they form a single package as far as Big 
Data processing—not just storage—is concerned. Tis chapter will treat them as one package. 
Today, Hadoop and/or MapReduce lack popular alternatives [2]. HDFS solves the practical 
problem of not being able to store Big Data on a single machine by distributing the storage over 
multiple nodes [3]. MapReduce is a framework on which one can run jobs that process the con-
tents of the storage—also in a distributed manner—and generate statistical summaries. Tis 
chapter will show that performance improvements mostly target MapReduce [4].

There are several fundamental problems with MapReduce. First, the map and reduce opera-
tors are restricted to key–value hashes (data type, not hash function), which places a cap on 
usability. For example, while data streaming is a good alternative for Big Data processing, 
MapReduce fails to accommodate the necessary data types [5]. Secondly, MapReduce jobs cre-
ate heterogeneous environments where jobs compete for the same resource with no guarantee 
of fairness [4]. Finally, MapReduce jobs, or HDFS for that matter, lack time awareness, while 
some algorithms might need to process data in their time sequence or using a time window.

The core premise of this chapter is to replace HDFS/MapReduce with a time-aware stor-
age and processing logic. Big Data is replayed along the timeline, and all the jobs get the 
time-ordered sequence of data items. Te major diference here is that the new method col-
lects all the jobs in one place—the node that replays data—while HDFS/MapReduce sends 
jobs to remote nodes so that data can be processed locally. Tis architecture is chosen for 
the sole purpose of accommodating a wide range of data streaming algorithms and the 
data types they create [5].
Te new method presented in this chapter can be viewed as a generic technology that 
exists independently of HDFS and MapReduce. Tis technology is one of many possible 
practical applications of the new method.
------------------------------------------------------------------------------------------------------------------
Locking provides protection against both high-level and low-level conflicts by enforcing atomicity 
among methods and code-blocks synchronized on the same object. Atomic actions are 
performed as units, without any interleaving of the actions of other threads. But, as discussed in § 
1.3.2 and in Chapter 2, too much locking can also produce liveness problems that cause programs to 
freeze up. Rather than exploring these issues in detail now, we'll rely on some simple default rules for 
writing methods that preclude interference problems: 
•  Always lock during updates to object fields. 
•  Always lock during access of possibly updated object fields. 
•  Never lock when invoking methods on other objects. ------------------------------------------------------------------------------------------------------------------

The most common reason for using a finally block is to ensure that cleanup occurs regardless of 
what happens within the try block. 

Staying safe
One strategy for safety is to never return from a non-private method in an inconsistent
state, and to never call any non-private method (and certainly not a method on any
other object) while in an inconsistent state. If this is combined with a way of protect-
ing the object (such as a synchronization lock or critical section) while it’s inconsis-
tent, the system can be guaranteed to be safe.

--------------------------------------------------------------------------------------------------
Concurrent Programming in Java

The number field is declared as 

volatile -- ensure visibility from other tasks/threads 
after it is computed (see § 2.2.7). Here and in subsequent examples, volatile fields are 
read and/or written only once per task execution, and otherwise held in local variables. This 
avoids interfering with potential compiler optimizations that are otherwise disabled when 
using volatile. 

 The use of volatile fields is much more common in lightweight parallel 
task frameworks than in general-purpose concurrent programming. Tasks usually do not 
require other synchronization and control mechanics, yet often need to communicate results 
via field access. The most common reason for using synchronized instead of 
volatile is to deal with arrays. Individual array elements cannot be declared as 
volatile. Processing arrays within synchronized methods or blocks is the simplest 
way to ensure visibility of array updates, even in the typical case in which locking is not 
otherwise required. An occasionally attractive alternative is instead to create arrays each of 
whose elements is a forwarding object with volatile fields. 










---------------------------------------------------------






Kafka Cookbook

	Kafka is a highly distributed messaging system that connects your data
ingestion system to your real-time or batch processing systems such as Storm, Spark, or
Hadoop. Kafka allows you to scale your systems very well in a horizontal fashion without
compromising on speed or efficiency. 
	the	real	power	of	Kafka	is	unlocked
when	it	is	run	in	the	cluster	mode	with	replication	and	the	topics	are	appropriately
partitioned.	








----------------------------------------


Akka is one of the most popular Actor Model frameworks that provide a complete 
toolkit and runtime for designing and building highly concurrent, distributed, and 
fault-tolerant, event-driven applications on the JVM. This chapter will walk you 
through the motivation and need for building an Akka toolkit.
As Java/Scala developers, we will see the usage of creating applications using the 
Akka Actor Model, which scales up and scales out seamlessly, and provides levels  
of concurrency, which is simply diffcult to achieve with the standard Java libraries.

Concurrent systems
When writing large concurrent systems, the traditional model of shared state 
concurrency makes use of changing shared memory locations. The system uses 
multithreaded programming coupled with synchronization monitors to guard 
against potential deadlocks. The entire multithreading programming model is based 
on how to manage and control the concurrent access to the shared, mutable state.
Manipulating shared, mutable state via threads makes it hard at times to  
debug problems. Usage of locks may guarantee the correct behavior, but it is  
likely to lead to the effect of threads running into a deadlock problem, with  
each acquiring locks in a different order and waiting for each other, as shown 
in the following diagram:
Working with threads requires a much higher level of programming skills and it is 
very diffcult to predict the behavior of the threads in a runtime environment.
Java provides shared memory threads with locks as the primary form of concurrency 
abstractions. However, shared memory threads are quite heavyweight and incur 
severe performance penalties from context-switching overheads.
A newer Java API around fork/join, based on work-stealing algorithms, makes the 
task easier, but it still takes a fair bit of expertise and tuning to write the application.






sidenote
Writing multithreaded applications that can take 
advantage of the underlying hardware is very  
error-prone and not easy to build.
Scaling up Java programs is diffcult; scaling out  
Java programs is even more diffcult.

----------------------------------------------------------------------


MapReduce (Dean and Ghemawat, 2004) provides a framework for performing a two-
phase distributed computation on large datasets, which in our case is a training dataset
D. In theMap phase, the system partitions D into a set of disjoint units that are assigned
to worker processes, known asmappers. Eachmapper (in parallel with the others) scans
through its assigned data and applies a user-speci?ed map function to each record. The
output of the map function is a set of key–value pairs that are collected by the Shuf?e
phase, which groups them by key. The master process redistributes the output of shuf?e
to a series of worker processes called reducers, which perform the Reduce phase. Each
reducer applies a user-speci?ed reduce function to all the values for a key and outputs
the value of the reduce function. The collection of ?nal values from all of the reducers
is the ?nal output of MapReduce.







--------------------------------------------------------------------------------

MIT
Reading 19: Concurrency
Software Construction -  6.005
http://web.mit.edu/6.005/www/fa15

Concurrency
Concurrency means multiple computations are happening at the same time. Concurrency is
everywhere in modern programming, whether we like it or not:

There are two common models for concurrent programming: shared memory and message passing.
Shared memory. In the shared memory model of concurrency, concurrent
modules interact by reading and writing shared objects in memory.
Message passing. In the message-passing model, concurrent modules interact by
sending messages to each other through a communication channel. Modules send off
messages, and incoming messages to each module are queued up for handling.

The message-passing and shared-memory models are about how concurrent modules
communicate. The concurrent modules themselves come in two different kinds: processes and
threads.
Process. A process is an instance of a running program that is isolated from other processes on
the same machine. In particular, it has its own private section of the machine’s memory.
The process abstraction is a virtual computer. It makes the program feel like it has the entire
machine to itself – like a fresh computer has been created, with fresh memory, just to run that
program.
Thread. A thread is a locus of control inside a running program. Think of it as a place in the
program that is being run, plus the stack of method calls that led to that place (so the thread can go
back up the stack when it reaches  return  statements).
Just as a process represents a virtual computer, the thread abstraction represents a virtual
processor. Making a new thread simulates making a fresh processor inside the virtual computer
represented by the process. This new virtual processor runs the same program and shares the
same memory as other threads in the process.
Threads are automatically ready for shared memory, because threads share all the memory in the
process. It takes special effort to get “thread-local” memory that’s private to a single thread. It’s also
necessary to set up message-passing explicitly, by creating and using queue data structures.

 When there are more threads than processors,
concurrency is simulated by time slicing, which means that the processor
switches between threads. 
On most systems, time slicing happens unpredictably and nondeterministically, meaning that a
thread may be paused or resumed at any time.

 A race condition means that the correctness of the
program (the satisfaction of postconditions and invariants) depends on the relative timing of events
in concurrent computations A and B. When this happens, we say “A is in a race with B.”
Some interleavings of events may be OK, in the sense that they are consistent with what a single,
nonconcurrent process would produce, but other interleavings produce wrong answers – violating
postconditions or invariants.
You can’t tell just from looking at Java code how the processor is going to execute it. You can’t tell
what the indivisible operations – the atomic operations – will be. It isn’t atomic just because it’s one
line of Java. It doesn’t touch balance only once just because the balance identifier occurs only once
in the line. The Java compiler, and in fact the processor itself, makes no commitments about what
low-level operations it will generate from your code. In fact, a typical modern Java compiler
produces exactly the same code for all three of these versions!
The key lesson is that you can’t tell by looking at an expression whether it will be safe from race
conditions.

Modules interact by sending
messages to each other. Incoming
requests are placed in a queue to be
handled one at a time. The sender doesn’t
stop working while waiting for an answer to
its request. It handles more requests from
its own queue. The reply to its request
eventually comes back as another
message.
Unfortunately, message passing doesn’t eliminate the possibility of race conditions.


Concurrency is Hard to Test and Debug
If we haven’t persuaded you that concurrency is tricky, here’s the worst of it. It’s very hard to
discover race conditions using testing. And even once a test has found a bug, it may be very hard
to localize it to the part of the program causing it.
Concurrency bugs exhibit very poor reproducibility. It’s hard to make them happen the same way
twice. Interleaving of instructions or messages depends on the relative timing of events that are
strongly influenced by the environment. Delays can be caused by other running programs, other
network traffic, operating system scheduling decisions, variations in processor clock speed, etc.
Each time you run a program containing a race condition, you may get different behavior.
These kinds of bugs are heisenbugs, which are nondeterministic and hard to reproduce, as
opposed to a bohrbug, which shows up repeatedly whenever you look at it. Almost all bugs in
sequential programming are bohrbugs.
A heisenbug may even disappear when you try to look at it with  println  or  debugger !  The
reason is that printing and debugging are so much slower than other operations, often 100-1000x
slower, that they dramatically change the timing of operations, and the interleaving. So inserting a
simple print statement into the cashMachine():

	private static void cashMachine() {
		for (int i = 0; i < TRANSACTIONS_PER_MACHINE; ++i) {
			deposit(); // put a dollar in
			withdraw(); // take it back out
			System.out.println(balance); // makes the bug disappear!
		}
	}
…and suddenly the balance is always 0, as desired, and the bug appears to disappear. But it’s only
masked, not truly fixed. A change in timing somewhere else in the program may suddenly make the
bug come back.
Concurrency is hard to get right. Part of the point of this reading is to scare you a bit. Over the next
several readings, we’ll see principled ways to design concurrent programs so that they are safer
from these kinds of bugs.

Summary
Concurrency: multiple computations running simultaneously
Shared-memory & message-passing paradigms
Processes & threads
Process is like a virtual computer; thread is like a virtual processor
Race conditions
When correctness of result (postconditions and invariants) depends on the relative
timing of events
These ideas connect to our three key properties of good software mostly in bad ways. Concurrency
is necessary but it causes serious problems for correctness. We’ll work on fixing those problems in
the next few readings.
Safe from bugs. Concurrency bugs are some of the hardest bugs to find and fix, and
require careful design to avoid.
Easy to understand. Predicting how concurrent code might interleave with other concurrent
code is very hard for programmers to do. It’s best to design your code in such a way that
programmers don’t have to think about interleaving at all.
Ready for change. Not particularly relevant here.
-------------------------------------------------------------------------------

MapReduce has its roots in functional programming
A key feature of functional languages is the concept of higher-order functions, or functions
that can accept other functions as arguments.Two common built-in higher order functions are map
and fold
Given a list, map takes as an argument a function f (that takes a
single argument) and applies it to all elements in a list (the top part of the diagram). Given a list
fold takes as arguments a function g (that takes two arguments) and an initial value: g is first applied
to the initial value and the first item in the list, the result of which is stored in an intermediate
variable.This intermediate variable and the next item in the list serve as the arguments to a second
application of g, the results of which are stored in the intermediate variable. This process repeat
until all items in the list have been consumed; fold then returns the final value of the intermediate
variable.

Typically,map and fold are used in combination.

We can view map as a concise way to represent the transformation of a dataset (as de?ned
by the function f ). In the same vein, we can view fold as an aggregation operation, as de?ned by
the function g. One immediate observation is that the application of f to each item in a list (or
more generally, to elements in a large dataset) can be parallelized in a straightforward manner, since
each functional application happens in isolation. In a cluster, these operations can be distributed
across many different machines. The fold operation, on the other hand, has more restrictions on
data locality—elements in the list must be “brought together” before the function g can be applied.
However, many real-world applications do not require g to be applied to all elements of the list.To
the extent that elements in the list can be divided into groups, the fold aggregations can also proceed
in parallel. Furthermore, for operations that are commutative and associative, significant ef?ciencies
can be gained in the fold operation through local aggregation and appropriate reordering.
In a nutshell, we have described MapReduce. The map phase in MapReduce roughly corre-
sponds to the map operation in functional programming, whereas the reduce phase in MapReduce
roughly corresponds to the fold operation in functional programming. As we will discuss in detail
shortly, theMapReduce execution framework coordinates the map and reduce phases of processing
over large amounts of data on large clusters of commodity machines

Viewed from a slightly different angle,MapReduce codifes a generic “recipe” for processing
large datasets that consists of two stages. In the first stage, a user-specified computation is applied
over all input records in a dataset. These operations occur in parallel and yield intermediate output
that is then aggregated by another user-specified computation. The programmer de?nes these two
types of computations, and the execution framework coordinates the actual processing (very loosely,
MapReduce provides a functional abstraction).



====================================================

http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html

What does volatile do?

Volatile fields are special fields which are used for communicating state between threads. Each read of a volatile will see the last write to that volatile by any thread; in effect, they are designated by the programmer as fields for which it is never acceptable to see a "stale" value as a result of caching or reordering. The compiler and runtime are prohibited from allocating them in registers. They must also ensure that after they are written, they are flushed out of the cache to main memory, so they can immediately become visible to other threads. Similarly, before a volatile field is read, the cache must be invalidated so that the value in main memory, not the local processor cache, is the one seen. There are also additional restrictions on reordering accesses to volatile variables.

Under the old memory model, accesses to volatile variables could not be reordered with each other, but they could be reordered with nonvolatile variable accesses. This undermined the usefulness of volatile fields as a means of signaling conditions from one thread to another.

Under the new memory model, it is still true that volatile variables cannot be reordered with each other. The difference is that it is now no longer so easy to reorder normal field accesses around them. Writing to a volatile field has the same memory effect as a monitor release, and reading from a volatile field has the same memory effect as a monitor acquire. In effect, because the new memory model places stricter constraints on reordering of volatile field accesses with other field accesses, volatile or not, anything that was visible to thread A when it writes to volatile field f becomes visible to thread B when it reads f. 


Effectively, the semantics of volatile have been strengthened substantially, almost to the level of synchronization. Each read or write of a volatile field acts like "half" a synchronization, for purposes of visibility.

Important Note: Note that it is important for both threads to access the same volatile variable in order to properly set up the happens-before relationship. It is not the case that everything visible to thread A when it writes volatile field f becomes visible to thread B after it reads volatile field g. The release and acquire have to "match" (i.e., be performed on the same volatile field) to have the right semantics. 


------------------------------------------
https://en.wikipedia.org/wiki/Volatile_(computer_programming)#cite_note-8


In computer programming, particularly in the C, C++, C#, and Java programming languages, the volatile keyword indicates that a value may change between different accesses, even if it does not appear to be modified. This keyword prevents an optimizing compiler from optimizing away subsequent reads or writes and thus incorrectly reusing a stale value or omitting writes. Volatile values primarily arise in hardware access (memory-mapped I/O), where reading from or writing to memory is used to communicate with peripheral devices, and in threading, where a different thread may have modified a value.

Despite being a common keyword, the behavior of volatile differs significantly between programming languages, and is easily misunderstood. In C and C++, it is a type qualifier, like const, and is a property of the type. Furthermore, in C and C++ it does not work in most threading scenarios, and that use is discouraged. In Java and C#, it is a property of a variable and indicates that the object to which the variable is bound may mutate, and is specifically intended for threading. In the D programming language, which is based on C++, there is a separate keyword shared for the threading usage, but no volatile keyword exists.

In Java

The Java programming language also has the volatile keyword, but it is used for a somewhat different purpose. When applied to a field, the Java qualifier volatile provides the following guarantees:

    In all versions of Java, there is a global ordering on the reads and writes to a volatile variable. This implies that every thread accessing a volatile field will read its current value before continuing, instead of (potentially) using a cached value. (However, there is no guarantee about the relative ordering of volatile reads and writes with regular reads and writes, meaning that it's generally not a useful threading construct.)
    In Java 5 or later, volatile reads and writes establish a happens-before relationship, much like acquiring and releasing a mutex.[8]

Using volatile may be faster than a lock, but it will not work in some situations.[citation needed] The range of situations in which volatile is effective was expanded in Java 5; in particular, double-checked locking now works correctly.[9]

[8] Section 17.4.4: Synchronization Order "The Java® Language Specification, Java SE 7 Edition". Oracle Corporation. 2013. Retrieved 2013-05-12.
[9] Neil Coffey. "Double-checked Locking (DCL) and how to fix it". Javamex. Retrieved 2009-09-19.
----------------------------------------

17.4.4. Synchronization Order

Every execution has a synchronization order. A synchronization order is a total order over all of the synchronization actions of an execution. For each thread t, the synchronization order of the synchronization actions (§17.4.2) in t is consistent with the program order (§17.4.3) of t.

Synchronization actions induce the synchronized-with relation on actions, defined as follows:

    An unlock action on monitor m synchronizes-with all subsequent lock actions on m (where "subsequent" is defined according to the synchronization order).

    A write to a volatile variable v (§8.3.1.4) synchronizes-with all subsequent reads of v by any thread (where "subsequent" is defined according to the synchronization order).

    An action that starts a thread synchronizes-with the first action in the thread it starts.

    The write of the default value (zero, false, or null) to each variable synchronizes-with the first action in every thread.

    Although it may seem a little strange to write a default value to a variable before the object containing the variable is allocated, conceptually every object is created at the start of the program with its default initialized values.

    The final action in a thread T1 synchronizes-with any action in another thread T2 that detects that T1 has terminated.

    T2 may accomplish this by calling T1.isAlive() or T1.join().

    If thread T1 interrupts thread T2, the interrupt by T1 synchronizes-with any point where any other thread (including T2) determines that T2 has been interrupted (by having an InterruptedException thrown or by invoking Thread.interrupted or Thread.isInterrupted).

The source of a synchronizes-with edge is called a release, and the destination is called an acquire. 



-----------------------------------

SCALA		SCALA		SCALA

Scala Design Patterns

SCALA

•     Provides byte code compatibility with the Java Virtual Machine (meaning that it 
is possible to integrate Scala applications with those written in Java and it is also 
possible to exploit the extensive range of libraries and frameworks developed for 
Java meaning that there is a huge set of resources available to the Scala 
developer) 
•    It incorporates strong typing without requiring a huge amount of Thing 
thing = new Thing(); thanks to extensive use of type inference 
•    It avoids a large amount of boilerplate code (for example setters and getters in 
Java). 
•    A concise language for expressing common concepts. 
•    A presumption of   immutability   that greatly enhances maintenance and future 
development. 
•    In corporates extensive pattern matching features making many conditional situations 
easier to write. 
 Of course that combination of Functional Programming and Object Orientation 
is also a massive plus which results in greatly simpli?  ed constructs and code if 
used judiciously.  


The name Scala is derived from Sca(lable) La(nguage) and is a multi-paradigm 
language, providing:
•     Object oriented concepts, such as classes, methods, inheritance, polymorphism, etc. 
•    That adds in (compared to Java at least) the concepts of traits and mixins. 
•    Functional concepts, such as functions as first class entities in the language, as 
well as concepts such as partially applied functions and currying which allow 
new functions to be constructed from existing functions. 
•    Static typing with type inference used whether ever possible 
•    Adopting a concise, but still expressive, syntax. 
•    With direct support within the language for XML    (that is rather than treating 
XML based facilities as a library to be invoked XML literals can be embedded 
directly into the Scala code). 
•    Concurrency based on the Actor/AKKA model. 
•    Interoperability with the Java/JVM world. 
•    Results in far fewer null pointer errors than occur in Java. One of the biggest 
issues in Java is what does the Null Pointer Exception relate to as that actual 
exception provides very little information. 
•    Scala allows for operator overloading, an omission from Java intended to keep 
things simple, but which often results in either more complexity and certainly in 
less readable code. 
•    Extensive list processing operations. Java and C# do include some list processing 
facilities but Scala takes this further and provides extensive support for operations 
that can be applied to collections of objects.  
•    Explicit support for the Singleton design pattern, which makes it easy (and natural) 
to use the Singleton pattern within applications.  
•    Allows for the creation of new syntax and control structures without using macros 
and still retaining strong static typing.  


In Scala case the boilerplate code of class construction is handled by the language
  class Person(var  firstName: String, var lastName: String, var age: Int) 
  
No static in Scala
Scala does away with this distinction by not including the static concept. 
Instead it allows the user to define singleton objects, if these singleton objects have 
the same name as a class and are in the same course file as the class, then they are 
referred to as companion objects.  Companion objects then have a special relationship
ship with the class that allows them to access the internals of a class and thus provide 
the Scala equivalent of static behavior. 

 The class hierarchy in Scala is based on single inheritance of classes but allows 
multiple traits to be mixed into any given class. A Trait is a structure within the 
Scala language that is neither a class nor an interface (note Scala does not have 
interfaces even though it compiles to Java Byte Codes). It can however, be combined 
with classes to create new types of classes and objects. As such a Trait can 
contain data, behavior, functions, type declarations, abstract members etc. but cannot 
be instantiated itself. 


 Patterns can be useful in situations where solutions to problems recur but in slightly 
different ways. Thus, the solution needs to be instantiated as appropriate for differ-
ent problems. The solutions should not be so simple that a simple linear series of 
instructions will suf?  ce. In such situations patterns are overkill. They are particu-
larly relevant when several steps are involved in the pattern that may not be required 
for all problems. Finally, patterns are really intended for solutions where the devel-
oper is more interested in the existence of the solution rather than how it was derived 
(as patterns still leave out too much detail). 

 Design patterns have a number of strengths including:
•     providing  a  common  vocabulary, 
•    explicitly capturing expert knowledge and trade-offs, 
•    helping to improve developer communication, 
•    promoting the ease of maintenance, 
•    providing  a  structure  for  change. 
 However, they are not without their limitations. These include:
•     not leading to direct code reuse, 
•    being  deceptively  simple, 
•    easy to get pattern overload (i.e. ?  nding the right pattern), 
•    they are validated by experience rather than testing, 
•    no  methodological  support. 


 In addition, once they have found the design that they feel best matches their 
needs, they must then consider how to apply it to their application. This is 
because a design pattern describes a solution to a particular design problem. This 
solution may include multiple tradeoffs which are contradictory and which the 
designer must choose between, although some aspects of the system structure 
can be varied independently.  


--------

   The  UML  defines a number of models and their notations:
•     Use case diagrams  organize the use cases that encompass a system’s behaviour 
(they are based on the use case diagrams of Objectory). 
•    Class diagrams  express the static structure of the system (they derive from the 
Booch and OMT methods), for example the   part-of  and   is-a   relationships 
between classes and objects. The class diagrams also encompass the object diagrams. 
Therefore, in this book, we refer to them as the Object Model (as in OMT). 
•    Sequence diagrams  (known as message-trace diagrams, in version 0.8 of the 
Unified Method draft) deal with the time ordered sequence of transactions between objects. 
•    Collaboration diagrams  (previously known as Object-message diagrams) indicate 
the order of messages between specified objects. They complement sequence 
diagrams as they illustrate the same information. Sequence diagrams highlight 
the actual sequence, while collaboration diagrams highlight the structure required 
to support the message sequence. 
•    State machine diagrams  are based on statecharts, like those in OMT. They capture 
the dynamic behaviour of the system.  
•    Component diagrams  (known as module diagrams, in version 0.8 of the Unified 
Method draft) represent the development view of the system. That is, how the 
system should be developed into software modules. You can also use them to 
represent concepts such as dynamic libraries. 
•    Deployment diagrams  (previously known as platform diagrams) attempt to capture 
the topology of the system once it is deployed. They reflect the physical 
topology upon which the software system is to execute. 





 Immutability indicates that once created, an objects’ data cannot be changed (this is 
in contrast to mutable objects where the state of the object can be changed). 
 The  benefits of using the Immutable pattern include:
•     Immutable objects are often easier to use. 
•    Immutable objects reduce the number of possible interactions (aliasing) between 
different parts of the program 
•    Immutable objects can be safely shared between multiple threads. 
•    Implementing an immutable objects is often easier, as there is less that can go 
wrong and the design space is “smaller” 
•    It’s easy to work with immutable objects in a functional language 
 The drawbacks of using the Immutable pattern include:
•     Immutable objects often result in more objects being created 
•    It may not be obvious how to add, remove or modify elements in an immutable object.  
•    It may not be obvious that the   add ,    delete ,    concatenate  etc. methods actually 
return a new instance which must be referenced in order to pick up the changed data.  
•    Immutable objects may actually add to the complexity of a piece of code (due to 
the above) rather than simplify it. 




 The Singleton pattern describes a type that can only have one object constructed for 
it. That is, unlike other objects it should not be possible to obtain more than one 
instance within the same virtual machine. Thus the Singleton pattern ensures that 
only one instance of a class is created. All objects that use an instance of that type 
use the same instance. 

----------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------
https://www.chrisstucchio.com/
https://www.chrisstucchio.com/blog/2013/actors_vs_futures.html
Don't use Actors for concurrency

A common practice I've seen in scala code is to use actors for concurrency. This is
encouraged by Akka and a lot of writing about Scala, the documentation of which is
highly actor-centric. I assert that this is a bad practice and should be considered an
antipattern most of the time it is used. Actors should not be used as a tool for flow
control or concurrency - they are an effective tool for two purposes, maintaining state
and providing a messaging endpoint. In all other circumstances, it is probaby better to
use Futures.
----------------------------------------------------------------------------------------






----------------------------------------------------------------------------------------
Accompanies Data Algorithms 
Introduction to MapReduce Mahmoud Parsian

H:\temp\Introduction-to-MapReduce.pdf

a design pattern is a language-independent reusable solution to a common problem
that enables us to produce reusable code
A design pattern is not a finished design that
can be transformed directly into source or machine code.

A typical MapReduce program for Hadoop platform will have the following
components:
• Driver Program
• Mapper Class
• Reducer Class

The map() function of a Mapper class maps input (key1, value1) pairs to a
set of (zero, one, or more) intermediate (key2, value2) pairs. According to
Hadoop: ”Maps are the individual tasks which transform input records
into a intermediate records. The transformed intermediate records need not
be of the same type as the input records. A given input pair may map to
zero or many output pairs.”

 
----------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------
 Hadoop
Application
Architectures

Note that anything you put in the Configuration object can be read through the
JobTracker (in MapReduce v1) or Application Manager (in YARN). These processes have
a web UI that is often left unsecured and readable to anyone with access to its URL, so we
recommend against passing sensitive information such as passwords through the
Configuration object. A better method is to pass the URI of a password file in HDFS,
which can have proper access permissions. The map and reduce tasks can then read the
content of the file and get the password if the user executing the MapReduce job has
sufficient privileges. 
 

If a combine() method is defined it can aggregate the values produced by the mapper. It
executes locally on the same node where the mapper executes, so this aggregation reduces
the output that is later sent through the network to the reducer. The reducer will still have
to aggregate the results from different mappers, but this will be over significantly smaller
data sets. It is important to remember that you have no control on whether the combiner
will execute. Therefore, the output of the combiner has to be identical in format to the
output of the mapper, because the reducer will have to process either of them. Also note
that the combiner executes after the output of the mapper is already sorted, so you can
assume that the input of the combiner is sorted.   
 

----------------------------------------------------------------------------------------   
 
 HH:\Ed\boox2print\HadoopinAction.pdf
 5.1.1  Chaining MapReduce jobs in a sequence 
Though you can execute the two jobs manually one after the other, it’s more convenient 
to automate the execution sequence. You can chain MapReduce jobs to run sequen­
tially, with the output of one MapReduce job being the input to the next. Chaining 
MapReduce jobs is analogous to Unix pipes  . 
		mapreduce-1 | mapreduce-2 | mapreduce-3 | ...


 5.1.2  Chaining MapReduce jobs with complex dependency 
Chaining MapReduce jobs sequentially is quite straightforward. Recall that a driver 
sets up a JobConf  object with the configuration parameters for a MapReduce job 
and passes the JobConf object to JobClient.runJob()   to start the job. As Job­
Client.runJob() blocks until the end of a job, chaining MapReduce jobs involves 
calling the driver of one MapReduce job after another. The driver at each job will 
have to create a new JobConf object and set its input path to be the output path of 
the previous job. You can delete the intermediate data generated at each step of the 
chain at the end. 

H:\temp\Wikipedia Book on Artificial Intelligence.pdf
Forward chaining starts with the available data and uses in-
ference rules to extract more data (from an end user, for
example) until a goal is reached. An inference engine using
forward chaining searches the inference rules until it ?nds
one where the antecedent (If clause) is known to be true.
When such a rule is found, the engine can conclude, or in-
fer, the consequent (Then clause), resulting in the addition
of new information to its data.
[1]



----------------------------------------------------------

Resilient Distributed Datasets-- A Fault-Tolerant Abstraction for In-Memory Cluster Computing


Resilient Distributed Datasets (RDDs), a distributed memory abstraction that allows programmers
to perform in-memory computations on large clusters while 
retaining the fault tolerance of data flow models like MapReduce.

WHY
Cause existing data flow systems [like MapReduce / Hadoop / Dryad] are ineffective and inefficient for
- iterative algorithms [which are used by Graph Applications and for Machine Learning], and
- interactive data mining tools  (where a user loads a dataset into RAM and runs ad-hoc queries).

Keeping all the data residing in memory improves performance significantly

To achieve fault tolerance efficiently, RDDs provide a highly restricted form of shared memory: 
they are read-only datasets that can only be constructed through bulk operations on other RDDs. 
RDDs are expressive enough to capture a wide class of computations, including MapReduce and specialized 
programming models for iterative jobs such as Pregel.

Hadoop / Dryad systems simplify distributed programming by automatically providing locality-aware scheduling, 
fault tolerance, and load balancing, enabling a wide range of users to analyze big datasets on commodity clusters.
But are inefficient where applications reuse the data ands perform interactive sub queries on residual 
resulting data sets.

Most current cluster computing systems are based on an acyclic data flow model, where records are loaded
from stable storage (e.g., a distributed file system), passed through a DAG of deterministic operators, and
written back out to stable storage. Knowledge of the data flow graph allows the runtime to automatically schedule
work and to recover from failures. While acyclic data flow is a powerful abstraction, there are applications 
that cannot be expressed efficiently using only this construct.

This class includes iterative algorithms commonly used in machine learning and graph
applications, which apply a similar function to the data on each step, and interactive data mining tools, where a
user repeatedly queries a subset of the data. Because data flow based frameworks do not explicitly provide support
for working sets, these applications have to output data to disk and reload it on each query with current systems,
leading to significant overhead.  
[working sets (i.e., applications that reuse an intermediate result in multiple parallel operations)]

Consistent writing to disk file io hampers performance in Hadoop / Dryad --- storing all the data in the memory in a 
partitioned fault tolerant manner is way better

Resilient Distributed Datasets (RDDs) support applications with working sets while retaining the attractive
properties of data flow models: automatic fault tolerance, locality-aware scheduling, and scalability. 

RDDs allow users to explicitly cache working sets in memory across queries, 
	leading to substantial speedups on future reuse.
RDDs provide a highly restricted form of shared memory: 
	they are read-only, partitioned collections of records that can only be created through 
	deterministic transformations (e.g., map, join and group-by) on other RDDs.
These restrictions, however, allow for low-overhead fault tolerance. 
In contrast to distributed shared memory systems, 
	which require costly checkpointing and rollback, 
RDDs reconstruct lost partitions through lineage: 
	an RDD has enough information about how it was derived from other RDDs to rebuild 
	just the missing partition, without having to checkpoint any data.
	Although RDDs are not a general shared memory abstraction, they
	represent a sweet-spot between expressivity, scalability
	and reliability, and we have found them well-suited for a wide range of data-parallel applications.
	
[ using the deterministic nature of RDDs to build 
	"rddbg", a debugging tool for Spark that lets users rebuild any RDD created 
		during a job using its lineage and rerun tasks on it in a conventional debugger.]	
	
RDDs are a more general abstraction for applications with working sets. 
They allow users to explicitly name and materialize intermediate results, 
control their partitioning, and use them in operations of their choice 
(as opposed to giving the runtime a set of MapReduce steps to loop). 


Spark provides a language-integrated programming interface similar to DryadLINQ
in the Scala programming language, making it easy for users to write parallel jobs. 
In addition, Spark can be used interactively to query big datasets from a
modified version of the Scala interpreter. 
Spark is the first system to allow an efficient, general-purpose programming language 
to be used interactively to analyze large datasets on clusters.

Spark outperforms Hadoop by 
	up to 20× for iterative applications, 
	improves the performance of a data analytics report by 40×, 
	and can be used interactively to scan a 1 TB dataset with latencies of 5–7s
	

In general, there are two options to make a distributed dataset fault-tolerant:
checkpointing the data or logging the updates made to it.	
	checkpointing is expensive: it would require replicating big datasets across machines over the 
		datacenter network, which typically has much lower bandwidth than the memory bandwidth within a machine
		and it would also consume additional storage (replicating data in RAM would reduce the total amount that
		can be cached, 
	while logging it to disk would slow down applications). 

	logging updates is also expensive if there are many of them. 

RDDs only support coarse-grained transformations, where we can log a single operation 
to be applied to many records. We then remember the series of transformations used to build an
RDD (i.e., its lineage) and use it to recover lost partitions.	
While supporting only coarse-grained transformations restricts the programming model, 
RDDs are suitable for a wide range of applications. In particular, 
RDDs are well-suited for data-parallel batch analytics applications, 
including data mining, machine learning, and graph algorithms, 
because these programs naturally perform the same operation on many records.

RDDs would be less suitable for applications that asynchronously update shared state, 
such as a parallel web crawler. 

Formally, an RDD is a read-only, partitioned collection of records. 
RDDs can be only created through deterministic operations 
	on either (1) a dataset in stable storage
	or (2) other existing RDDs. 
	
We call these operations transformations to differentiate them from other 
operations that programmers may apply on RDDs. 
Examples of transformations include map, filter, groupBy and join.
RDDs do not need to be materialized at all times. Instead, an RDD has enough information about how it was
derived from other datasets (i.e., its lineage) to compute its partitions from data in stable storage.

In Spark, RDDs are represented by objects, and transformations are invoked using methods on these objects.
After defining one or more RDDs, programmers can use them in actions, which are operations that return aIn Spark, RDDs are represented by objects, and transformations are invoked using methods on these objects.
After defining one or more RDDs, programmers can use them in "actions", 
which are operations that return a value to the application or export data to a storage system

actions:
	count 		returns the number of elements in the RDD, 
	collect 	returns the elements themselves
	save 		outputs the RDD to a storage system.

===========================================	
from 'Spark for DataScience' Srinivas Duvvuri,Bikramaditya Singhal

Driver program
The Spark shell is an example of a driver program. A driver program is a process that
executes in the JVM and runs the user's main function on it. It has a SparkContext object
which is a connection to the underlying cluster manager. A Spark application is initiated
when the driver starts and it completes when the driver stops. The driver, through an
instance of SparkContext, coordinates all processes within a Spark application.

Primarily, an RDD lineage Directed Acyclic Graph (DAG) is built on the driver side with
data sources (which may be RDDs) and transformations. This DAG is submitted to the
DAG scheduler when an action method is encountered. The DAG scheduler then splits the
DAG into logical units of work (for example, map or reduce) called stages. Each stage, in
turn, is a set of tasks, and each task is assigned to an executor (worker) by the task
scheduler. Jobs may be executed in FIFO order or round robin, depending on the
configuration.
	Inside a single Spark application, multiple parallel jobs can run
	simultaneously if they were submitted from separate threads.
	
SparkContext
SparkContext is the entry point to the Spark core engine. This object is required to create
and manipulate RDDs and create shared variables on a cluster. The SparkContext object
connects to a cluster manager, which is responsible for resource allocation. Spark comes
with its own standalone cluster manager. Since the cluster manager is a pluggable
component in Spark, it can be managed through external cluster managers such as Apache
Mesos or YARN.
When you start a Spark shell, a SparkContext object is created by default for you. You can
also create it by passing a SparkConf object that is used to set various Spark configuration
parameters as key value pairs. Please note that there can be only one SparkContext object in
a JVM.	

Worker nodes
Worker nodes are the nodes that run the application code in a cluster, obeying the driver
program. The real work is actually executed by the worker nodes. Each machine in the
cluster may have one or more worker instances (default one). A worker node executes one
or more executors that belong to one or more Spark applications. It consists of a block
manager component, which is responsible for managing data blocks. The blocks can be
cached RDD data, intermediate shuffled data, or broadcast data. When the available RAM is
not sufficient, it automatically moves some data blocks to disk. Data replication across
nodes is another responsibility of block manager.

Executors
Each application has a set of executor processes. Executors reside on worker nodes and
communicate directly with the driver once the connection is made by the cluster manager.
All executors are managed by SparkContext. An executor is a single JVM instance that
serves a single Spark application. An executor is responsible for managing computation
through tasks, storage, and caching on each worker node. It can run multiple tasks
concurrently.

Shared variables
Normally, the code is shipped to partitions along with separate copies of variables. These
variables cannot be used to propagate results (for example, intermediate work counts) back
to the driver program. Shared variables are used for this purpose. There are two kinds of 
shared variables, broadcast variables and accumulators.
Broadcast variables enable the programmers to retain a read-only copy cached on each node
rather than shipping a copy of it with tasks. If large, read-only data is used in multiple
operations, it can be designated as broadcast variables and shipped only once to all worker
nodes. The data broadcast in this way is cached in serialized form and is deserialized before
running each task. Subsequent operations can access these variables along with the local
variables moved along with the code. Creating broadcast variables is not necessary in all
cases, except the ones where tasks across multiple stages need the same read-only copy of
the data.
Accumulators are variables that are always incremented, such as counters or cumulative
sums. Spark natively supports accumulators of numeric types, but allows programmers to
add support for new types. Please note that the worker nodes cannot read the value of
accumulators; they can only modify their values.

















-----------------------------------------------------------
------------------------------------------------------------------
 Learning Spark	
		Holden Karau, Andy Konwinski, 
		Patrick Wendell & Matei Zaharia
		LIGHTNING-FAST DATA ANALYSIS




Spark’s shells allow you to interact with data that is dis-
tributed on disk or in memory across many machines, and Spark takes care of auto-
matically distributing this processing.
Because Spark can load data into memory on the worker nodes, many distributed
computations, even ones that process terabytes of data across dozens of machines,
can run in a few seconds. This makes the sort of iterative, ad hoc, and exploratory
analysis commonly done in shells a good fit for Spark.

In Spark, we express our computation through operations on distributed collections
that are automatically parallelized across the cluster. These collections are called resil-
ient distributed datasets, or RDDs. RDDs are Spark’s fundamental abstraction for dis-
tributed data and computation.

We can run various parallel operations on the RDD

every Spark application consists of a  driver program  that launches
various parallel operations on a cluster. The driver program contains your applica-
tion’s main function and defines distributed datasets on the cluster, then applies oper-
ations to them.

Driver programs access Spark through a  SparkContext object, which represents a
connection to a computing cluster. In the shell, a SparkContext is automatically
created for you as the variable called sc.

Spark automatically takes your function and ships
it to executor nodes. Thus, you can write code in a single driver program and auto-
matically have parts of it run on multiple nodes.

An RDD in Spark is simply an immutable distributed collection of objects. Each RDD
is split into multiple  partitions, which may be computed on different nodes of the
cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-
defined classes.

.................

Lazy evaluation means that when we call a transformation on an RDD (for instance,
calling map()), the operation is not immediately performed. Instead, Spark internally
records metadata to indicate that this operation has been requested. Rather than
thinking of an RDD as containing specific data, it is best to think of each RDD as
consisting of instructions on how to compute the data that we build up through
transformations. Loading data into an RDD is lazily evaluated in the same way trans-
formations are. So, when we call sc.textFile(), the data is not loaded until it is nec-
essary. As with transformations, the operation (in this case, reading the data) can
occur multiple times.
Spark uses lazy evaluation to reduce the number of passes it has to take over our data
by grouping operations together. In systems like Hadoop MapReduce, developers
often have to spend a lot of time considering how to group together operations to
minimize the number of MapReduce passes. In Spark, there is no substantial benefit
to writing a single complex map instead of chaining together many simple opera-
tions. Thus, users are free to organize their program into smaller, more manageable
operations.

....................

Sometimes we want to produce multiple output elements for each input element. The
operation to do this is called flatMap(). As with map(), the function we provide to
flatMap() is called individually for each element in our input RDD. Instead of
returning a single element, we return an iterator with our return values. Rather than
producing an RDD of iterators, we get back an RDD that consists of the elements
from all of the iterators. A simple usage of flatMap() is splitting up an input string
into words You can think of flatMap() as “flattening” the iterators returned to it, 
so that instead of ending up with an RDD of lists we have an RDD of the elements in those lists.

...........


Spark provides two ways to create RDDs: loading an external dataset and paralleliz-
ing a collection in your driver program.
The simplest way to create RDDs is to take an existing collection in your program
and pass it to SparkContext’s  parallelize() method







------------ java - the legend -------------
Simple Type System
Java differentiates between two types of values. These are object ref-
erences, and the eight primitive types (boolean, byte, short, char, int,
long, float, and double), which are not objects but merely immutable
data items.
This split is relatively simple, with the type system for objects
being single-rooted (everything ultimately inherits from
java.lang.Object) and single-inherited (every class has only one
parent class).
....
Unfortunately, this was largely too late for Java, as backwards com-
patibility of the language and type system, and the sheer volume of
existing Java code made moving to “everything is an object” simply
impractical. Java instead implemented a partial solution—the auto-
matic “boxing and unboxing” of primitive values to objects belong-
ing to a specific partner class of each primitive type. This was rea-
sonably successful, but came at the cost of additional complexity
and also exposed previously rather obscure debates about object
identity, and forced many more programmers to think about them.
In Java’s reference type system, the approach to object-oriented pro-
gramming started out reasonably simple. All classes have but a
single parent class, and all classes ultimately inherit from
java.lang.Object. As this form of inheritance is rather inflexible,
Java 1.0 also introduced a separate concept, the interface, which pro-
vides a specification for functionality that a class may advertise that
it offers.
If a class wants to declare that it offers functionality compatible with
an interface, it does so via the  class Foo implements
Functionality construct. In Java 1.0 until Java 7, the class was then
required to provide implementation code for every method declared
in the interface. Interface files were not allowed to provide any
method bodies—merely signatures and names of methods that
formed part of the collection of functionality that must be imple-
mented to conform to the interface
This model of types was significantly extended in several different
directions in Java 5. Firstly, the notion of the typesafe constant, or
enumeration was introduced via the  enum keyword. This enabled
developers to indicate that a particular type only had a finite and
known number of possible, and constant, values
Java’s enum constants are stronger than predecessors because they
are true types that inherit from  Object. This facility relies upon
additional language machinery to ensure that only the specified
instances can ever exist.
Enums were a very useful addition to the Java type system, and rep-
resented a significant upgrade to previous approaches. However, it is
important to note that compared to Scala and other languages with
more advanced type systems, Java only allows the representation of
disjoint alternatives as instances, rather than as types.

Java 5 introduced
annotations, as a way of exhibiting additional metadata which, while
relevant, was essentially independent of the functionality of the type.
This is sometimes referred to as “orthogonal” type information.
Annotations are fundamentally more flexible than interfaces
(although the mechanism makes use of them). Instead, classes,
methods, and even packages can be annotated with additional infor-
mation. For example, if a method is intended to be the end point for
a web services (REST) call, then appropriate information and sup-
port can be provided automatically by the web container hosting the
service.
The Java language has separate primitive and reference types (which
point at objects contained in Java’s heap). This means that a pro-
grammer’s code should always know the type of any expression.
This manifests itself in the JVM—specifically in the principle that
the interpretation of any given bit pattern differs, depending on
whether the pattern has the type of an  int, a  float, or a heap
address.
Accordingly, JVM bytecode is typed and the types are essentially
those of the Java language. The result is that most JVM bytecodes
fall into “families” that have several individual opcodes that deal
with the specific circumstances that are encountered in normal cod-
ing.
The most obvious sign of Java resigning its privileged place in the
firmament of JVM languages came with Java 7, when the
invokedynamic bytecode was added to the standard. At that time,
the reference implementation Java language compiler, Oracle’s
javac, would not under any circumstances emit an invokedynamic
opcode. Here was an opcode that had been added purely for the
benefit of non-Java languages, with no reference to Java whatsoever.
...
Arrays and Collections
Java 1.0 had two different types of data structures that shipped with
the platform—arrays, which were provided within the syntax of the
language, and some classes (such as  Hashtable and  Vector) that
resided within the java.util package of the Java standard library.
This represented a compromise. Programmers expected a familiar
C-like syntax for handling arrays, especially arrays of primitive
types. On the other hand, as heap-managed data, arrays were defi-
nitely objects in the Java worldview. This resulted in a halfway
house, where the object nature of arrays was somewhat sidelined,
and set up a rather obvious disconnect between seemingly simple
arrays and the object-oriented view contained in the classes of
java.util.
With the arrival of Java 1.2 (or Java 2 as the marketing went), Java
received a major upgrade in the form of the Java Collections libra-
ries. This was a full set of data structures covering the most common
cases, such as lists, sets, and maps. However, with the increased
emphasis on the object-oriented approach, the split between arrays
and object-based data structures became even more obvious.
One of the biggest changes to the language was Java Generics, which
appeared as a part of Java 5. Generics enable the programmer to rep-
resent a composite type, usually thought of as comprising a “con-
tainer” and “payload” type. Before Java 5, the programmer could
only discuss a type as  Box without any reference to what the box
contained. Using Java Generics, however, the type could be further
qualified to Box<Egg> or Box<Wine> or Box<Shoe>. This provides for
improved safety when programming, as the source compiler can
now detect mistaken attempts to put wine into objects that are really
shoeboxes or eggboxes.
....

The overall aim was not for lambda expressions per se, but rather to
evolve Java’s collections to allow support for “more functional” oper-
ations, such as map and filter. This was problematic, due to Java’s
requirement for backwards compatibility.
One of Java’s language constraints that arises from compatibility is
that Java interfaces may not have new methods added to them in a
new release of the interface. This is because if methods were to be
added, existing implementations of the interface would not have
those methods. That would mean older implementations would not
be seen as a valid implementation of the new version of the inter-
face, and so binary backwards compatibility would be broken.
This could not be allowed to happen, and so a different path was
chosen—allowing an interface to specify a default implementation
for new methods. With this change, new methods can be added to
interfaces—provided that they are default methods. Older imple-
mentations of interfaces, which do not have an implementation of
the new method, simply use the default provided by the interface
definition.
This change had the side effect of changing Java’s model of object-
oriented programming. Before Java 8, the model was strictly single-
inherited (for implementation) with interfaces providing an addi-
tional way for types to express their compatibility with a capability.
This was sometimes criticized for forcing types to repeat implemen-
tation code unecessarily.
With Java 8, however, Java’s model of objects changes, to allow mul-
tiple inheritance, but only of implementation. This is not the full
multiple inheritance of state as implemented by C++ or Scala.
Instead, it can be thought of as essentially being a form of “stateless
trait” or a way to implement the mixin pattern.

---
Finalization
Probably the worst feature in Java. The original intention was to
provide a method for automatically closing resources when they
were no longer needed (in a similar spirit to the C++ RAII pattern).
However, the mechanism relies upon Java’s garbage collection,
which is non-deterministic. Thus, using finalization to reclaim
resources is fundamentally unsafe (as the developer does not know
how long it will be until a resource is freed). Therefore it is impossi-
ble to use finalization as a way of avoiding resource exhaustion, and
the feature cannot be fixed. In other words, never use finalization.
Instead, Java 7 introduced “try-with-resources” as a way of automat-
ically controlling and closing resources that actually satisfies the
needs of Java programmers.


---------

javac does not recognize and eliminate tail recursion.
So this code:
    public static void main(String[] args) {
        int i = inc(0, 1_000_000_000);
        System.out.println(i);
    }
    private static int inc(int i, int iter) {
        if (iter > 0)
            return inc(i+1, iter-1);
		else
            return i;
    }
will cause a stack overflow if run. The equivalent Scala code, how-
ever, would run fine, because scalac does a great deal of optimiza-
tion at compile time, and will optimize away the tail recursion.
The general philosophy is that the JIT compiler is the part of the sys-
tem best able to cope with optimizing code. So javac allows the JIT
compiler free reign to apply complex optimizations (although this
does not include tail recursion elimination).

-----
-----
Modules
If lambda expressions were the “headline” feature for Java 8, in Java
9 it is anticipated to be modules. Up until now, the largest grouping
construct for Java code was a package, but the release of Java 9 will
see a new concept—the module. Modules are collections of code
that are larger than packages, and are no longer delivered as JAR
files (which are really just .zip files). Instead, modules have a new file
format that has been designed to be more efficient.
Modules also add a major new feature to the language, which is the
ability to enforce access control across modules. That is, modules
are able to fully specify their public API, and prevent access to pack-
ages that are only for internal use.
The ability for modules to allow internals access only to trusted cli-
ent code will have major repercussions for Java applications. This is
most apparent in the removal of access to a class called
sun.misc.Unsafe. This class is an internal class (as can be seen by
the fact that it lives in a sun package, rather than a java or javax
package) and should not be used directly by applications or libraries.
Unsafe contains functionality that enables low-level access to plat-
form features that are normally inaccessible to ordinary Java code. It
also contains code to directly access processor features, compare-
and-swap hardware for example. These capabilities are not part of
the Java standard, yet are extremely useful. The JDK class libraries
make heavy use of Unsafe, especially in places such as the concur-
rency classes.
----
Change Default Garbage Collector
The current default garbage collector is the parallel collector. The
parallel collector is extremely efficient, designed for high-
throughput operation and uses very small amounts of CPU time to
collect memory. However, the collector must pause the JVM to run a
garbage collection cycle (sometimes called a “Stop The World”
(STW) operation). These pauses typically last for up to a few hun-
dred milliseconds on heaps of 8 GB or less.
In Java 9, Oracle proposes to change the default collector to the new
Garbage First (G1) collector. This uses a more modern GC algo-
rithm that can do some of its work without pausing fully. The aim is
to let users set “pause goals” that the JVM will try to adhere to. How-
ever, G1 has some drawbacks: it uses much more CPU time overall
to collect memory, and still has the possibility of a significant pause.
By default, G1 will try to pause for no more than 200ms, unless nec-
essary, which isn’t necessarily a huge improvement over parallel.
G1 is also lacking in real-world testing. Despite being available since
Java 7, relatively few Java shops have adopted it, so the true impact
of changing the default collector is unknown. Applications that run
without an explicit choice of collector will be affected by a change of
default. Limited research has been done into the percentage of appli-
cations that would potentially be affected, but indications are that it
could over 50%.
------

---
The group summarized some of the key properties of HTTP/2 as
follows:
• Same HTTP APIs
• Cheaper requests
• Network- and server-friendliness
• Cache pushing
• Being able to change your mind
• More encryption

HTTP/2 responses can be interleaved, connections are not closed
unless a browser actively navigates away, and HTTP headers are
now represented in binary to avoid penalizing small requests and
responses (which is the majority of traffic).

---
 Java 8 introduced the Nashorn implementation of Java-
script on top of the JVM, and included the  jjs REPL. Due to
Nashorn’s tight integration with Java, this could be a useful environ-
ment for playing with Java in an interactive manner. However, it still
wasn’t Java.
As part of the development of Java 9, Project Kulla was started, to
look at producing a Java REPL that would provide as close an expe-
rience to “full Java” as possible. The project had some strict goals,
such as not to introduce new non-Java syntax. Instead, it disables
some features of the language that are not useful for interactive
development in order to provide a less awkward working environ-
ment.
In JShell, statements and expressions are evaluated immediately in
the context of an execution state. This means that they do not have
to be packaged into classes, and methods can also be free-standing.
JShell uses “snippets” of code to provide this top-level execution
environment.
--------------------------------------------



Big Data Benchmarks,
Performance Optimization,
and Emerging Hardware
6th Workshop, BPOE 2015
Kohala, HI, USA, August 31 – September 4, 2015
Revised Selected Papers
			Jianfeng Zhan • Rui Han • Roberto V. Zicari (Eds.)

BigDataBenchmarking---extract
2.2 Shark and Spark SQL
Shark is a query processing layer built on the Spark engine. Shark provides
in-memory columnar storage and columnar compression to store and process
relational data efficiently. For query processing, Shark uses a three-step similar
as traditional RDBMS, which includes query parsing, logical plan generation
and physical plan generation. Shark uses Hive query compiler directly for query
parsing and generates an abstract syntax tree (AST) for each query. The AST is
then translated into a logical plan consisting of operators and is later optimized
through some basic rules. Shark compiles the logical plan to create a physical
plan composed of transformations on RDDs. The physical plan is then submitted
to the Spark master as a Spark application and is executed by the engine.
Even though Shark provides the ability of SQL query processing on Spark,
there are still some limitations of it. For example, the logical plan generated by
Hive is originally designed for Hadoop MapReduce, which is not very suitable
for the Spark engine. Results computed from Shark also can not be used by
applications of other types directly, which makes it difficult to perform more
complex analytics. Spark SQL is developed as a module of Spark framework.
Spark SQL introduces a new abstraction called DataFrame to support the wide
range of data sources and algorithms. A DataFrame is a distributed collection
similar to a table in a relational database, with data from either external sources
or Spark’s built-in distributed collections. Users can then manipulate records in
Spark is a cluster computing framework based on the abstraction of resilient
distributed dataset (RDD). An RDD represents a read-only collection of objects
partitioned across a set of machines, which can be rebuilt when a partition
is lost. Spark provides the interface for users to decide whether to cache an
RDD across machines. This feature makes it easy to reuse data which makes
Spark well-suited for iterative jobs. For fault tolerance, each RDD records the
transformations used to build it. The RDD then contains enough information
about how it is derived from other RDDs, thus if a partition is lost, this RDD
can recompute just that partition to recover the lost data.
RDD have two types of operations: transformation and action. The trans-
formation operations, such as map, ?lter, and join, represent how an RDD is
created from data in stable storage or other RDDs. The action operations are
the operations on one RDD and return a value, such as collect, count, and save.
As mentioned above, the transformations on each RDD is stored as a set of
dependencies on parent RDDs. The dependencies are classi?ed into two types:
narrow dependencies where each partition of a parent RDD is used by at most
one partition of the child RDD, and wide dependencies where multiple child
partitions depend on one parent partition.
For job execution, Spark adopts a delay scheduling mechanism, which means
a job will not be submitted to the driver unless an action operator is called.
This mechanism enables Spark to perform optimizations before a job starts to
execute. When a job is submitted, it is transferred from a RDD’s lineage graph
to a directed acyclic graph (DAG) of stages for execution. Wide dependencies
(i.e., shuffle dependencies) with shuffe operations are spilt into di?erent stages,
while the narrow dependencies are computed in a pipelined way in one stage.
The scheduler then launch tasks of each stage on each machine to compute the
missing partitions until the target RDD has been computed. The task scheduler
assigns tasks based on data locality, which means to send each task to the node
containing the data partition. Results of each job are ?nally sent back to the
driver or stored in a storage system according to the action operation.

2.2 Shark and Spark SQL
Shark is a query processing layer built on the Spark engine. Shark provides
in-memory columnar storage and columnar compression to store and process
relational data e?ciently. For query processing, Shark uses a three-step similar
as traditional RDBMS, which includes query parsing, logical plan generation
and physical plan generation. Shark uses Hive query compiler directly for query
parsing and generates an abstract syntax tree (AST) for each query. The AST is
then translated into a logical plan consisting of operators and is later optimized
through some basic rules. Shark compiles the logical plan to create a physical
plan composed of transformations on RDDs. The physical plan is then submitted
to the Spark master as a Spark application and is executed by the engine.
Even though Shark provides the ability of SQL query processing on Spark,
there are still some limitations of it. For example, the logical plan generated by
Hive is originally designed for Hadoop MapReduce, which is not very suitable
for the Spark engine. Results computed from Shark also can not be used by
applications of other types directly, which makes it di?cult to perform more
complex analytics. Spark SQL is developed as a module of Spark framework.
Spark SQL introduces a new abstraction called DataFrame to support the wide
range of data sources and algorithms. A DataFrame is a distributed collection
similar to a table in a relational database, with data from either external sources
or Spark’s built-in distributed collections. Users can then manipulate records in
this DataFrame using Spark’s procedural API or relational APIs provided by
Spark SQL. Spark SQL also supports nested data model from Hive for tables for
Hive sql compatibility.
For query processing, Spark SQL uses a new component called Catalyst
instead of the Hive compiler for logical plan generating and optimizing. Catalyst
is an extensible optimizer which provides the convenience for external developers
to add new optimization rules and features. Similar to Shark, a query in Spark
SQL is first translated into an AST, and then into a logical plan. The logical
plan is then optimized by Catalyst with some rule-based optimizations, such as
constant folding, predicate pushdown. Spark SQL generates one or more physi-
cal plans for one optimized logical plan, and uses a cost model to select one for
processing. Spark SQL also adopts code generation to optimize the final plan for
better performance.
---------------------------------------------------------------------------


How Data Volume A?ects Spark Based Data
Analytics on a Scale-up Server
.............

7 Conclusions
We have reported a deep dive analysis of Spark based data analytics on a large
scale-up server. The key insights we have found are as follows:
– Spark workloads do not benefit significantly from executors with more than
12 cores.
– The performance of Spark workloads degrades with large volumes of data due
to substantial increase in garbage collection and file I/O time.
– With out any tuning, Parallel Scavenge garbage collection scheme outperforms
Concurrent Mark Sweep and G1 garbage collectors for Spark workloads.
– Spark workloads exhibit improved instruction retirement due to lower L1
cache misses and better utilization of functional units inside cores at large
volumes of data.
– Memory bandwidth utilization of Spark benchmarks decreases with large volumes
of data and is 3x lower than the available o?-chip bandwidth on our test
machine.

We conclude that Spark run-time needs node-level optimizations to maximize
its potential on modern servers. Garbage collection is detrimental to performance
of in-memory big data systems and its impact could be reduced by careful
matching of garbage collection scheme to workload. Inconsistencies in micro-
architecture performance across the data sizes pose additional challenges for
computer architects. O?-chip memory buses should be optimized for in-memory
data analytics workloads by scaling back unnecessary bandwidth.





==================


fromitsreamed.md
# Apache Flink

Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities.

Learn more about Flink at [http://flink.apache.org/](http://flink.apache.org/)


### Features

* A streaming-first runtime that supports both batch processing and data streaming programs

* Elegant and fluent APIs in Java and Scala

* A runtime that supports very high throughput and low event latency at the same time

* Support for *event time* and *out-of-order* processing in the DataStream API, based on the *Dataflow Model*

* Flexible windowing (time, count, sessions, custom triggers) accross different time semantics (event time, processing time)

* Fault-tolerance with *exactly-once* processing guarantees

* Natural back-pressure in streaming programs

* Libraries for Graph processing (batch), Machine Learning (batch), and Complex Event Processing (streaming)

* Built-in support for iterative programs (BSP) in the DataSet (batch) API

* Custom memory management for efficient and robust switching between in-memory and out-of-core data processing algorithms

* Compatibility layers for Apache Hadoop MapReduce and Apache Storm

* Integration with YARN, HDFS, HBase, and other components of the Apache Hadoop ecosystem
